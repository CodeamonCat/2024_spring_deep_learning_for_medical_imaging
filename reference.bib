@article{ALDHABYANI2020104863,
  title    = {Dataset of breast ultrasound images},
  journal  = {Data in Brief},
  volume   = {28},
  pages    = {104863},
  year     = {2020},
  issn     = {2352-3409},
  doi      = {https://doi.org/10.1016/j.dib.2019.104863},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352340919312181},
  author   = {Walid Al-Dhabyani and Mohammed Gomaa and Hussien Khaled and Aly Fahmy},
  keywords = {Ultrasound, Breast cancer, Medical images, Dataset, Deep learning, Classification, Segmentation, Detection},
  abstract = {Breast cancer is one of the most common causes of death among women worldwide. Early detection helps in reducing the number of early deaths. The data presented in this article reviews the medical images of breast cancer using ultrasound scan. Breast Ultrasound Dataset is categorized into three classes: normal, benign, and malignant images. Breast ultrasound images can produce great results in classification, detection, and segmentation of breast cancer when combined with machine learning.}
}


@inproceedings{7298594,
  author    = {Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Going deeper with convolutions},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {1-9},
  keywords  = {Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision},
  doi       = {10.1109/CVPR.2015.7298594}
}


@inproceedings{7780459,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {770-778},
  keywords  = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi       = {10.1109/CVPR.2016.90}
}

@inproceedings{10.1145/3450439.3451867,
  author    = {Ke, Alexander and Ellsworth, William and Banerjee, Oishi and Ng, Andrew Y. and Rajpurkar, Pranav},
  title     = {CheXtransfer: performance and parameter efficiency of ImageNet models for chest X-Ray interpretation},
  year      = {2021},
  isbn      = {9781450383592},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3450439.3451867},
  doi       = {10.1145/3450439.3451867},
  abstract  = {Deep learning methods for chest X-ray interpretation typically rely on pretrained models developed for ImageNet. This paradigm assumes that better ImageNet architectures perform better on chest X-ray tasks and that ImageNet-pretrained weights provide a performance boost over random initialization. In this work, we compare the transfer performance and parameter efficiency of 16 popular convolutional architectures on a large chest X-ray dataset (CheXpert) to investigate these assumptions. First, we find no relationship between ImageNet performance and CheXpert performance for both models without pretraining and models with pretraining. Second, we find that, for models without pretraining, the choice of model family influences performance more than size within a family for medical imaging tasks. Third, we observe that ImageNet pretraining yields a statistically significant boost in performance across architectures, with a higher boost for smaller architectures. Fourth, we examine whether ImageNet architectures are unnecessarily large for CheXpert by truncating final blocks from pretrained models, and find that we can make models 3.25x more parameter-efficient on average without a statistically significant drop in performance. Our work contributes new experimental evidence about the relation of ImageNet to chest x-ray interpretation performance.},
  booktitle = {Proceedings of the Conference on Health, Inference, and Learning},
  pages     = {116–124},
  numpages  = {9},
  keywords  = {truncation, pretraining, generalization, efficiency, chest X-ray interpretation, ImageNet},
  location  = {Virtual Event, USA},
  series    = {CHIL '21}
}

@inproceedings{NIPS2012_c399862d,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}


@misc{gheflati2022vision,
  title         = {Vision Transformer for Classification of Breast Ultrasound Images},
  author        = {Behnaz Gheflati and Hassan Rivaz},
  year          = {2022},
  eprint        = {2110.14731},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{kornblith2019better,
  title         = {Do Better ImageNet Models Transfer Better?},
  author        = {Simon Kornblith and Jonathon Shlens and Quoc V. Le},
  year          = {2019},
  eprint        = {1805.08974},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{simonyan2015deep,
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author        = {Karen Simonyan and Andrew Zisserman},
  year          = {2015},
  eprint        = {1409.1556},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}